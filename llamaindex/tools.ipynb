{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tools\n",
    "## FunctionTools\n",
    "FunctionTool allows to convert any Python function to a tool."
   ],
   "id": "c7fa5622290f4ece"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"\n",
    "    Dummy method to get the weather for a given location.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is sunny.\"\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\"\n",
    ")\n",
    "tool.call(\"New York\")"
   ],
   "id": "7f709aee398b01a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## QueryEngineTool\n",
    "A QueryEngine can be transformed into a tool using the `QueryEngineTool` class."
   ],
   "id": "cc30196b3357c603"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "import chromadb\n",
    "\n",
    "# this code is for HuggingFace\n",
    "# from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI\n",
    "# from llama_index.embedding.huggingface import HuggingFaceEmbedding\n",
    "# embed_model = HuggingFaceEmbedding(\"BAAI/bge-small-en-v1.5\")\n",
    "# llm = HuggingFAceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)\n",
    "\n",
    "llm = Ollama(model=\"qwen2.5-coder\")\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "tool = QueryEngineTool.from_defaults(query_engine, \n",
    "                                     name=\"Some useful tool name\", \n",
    "                                     description=\"Some useful tool description\")\n",
    "\n",
    "tool.call(\"Respond using a persona that describes author and travel experiences?\")"
   ],
   "id": "a27ad95311dc9d25",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## ToolSpecs\n",
    "`ToolSpecs` are collection of tools that work together harmoniously developed and maintained by the community. A `ToolSpec` combines related tools for specific purposes."
   ],
   "id": "ed64492c65015605"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "\n",
    "# look at the metadata of each tool\n",
    "[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ],
   "id": "143305dddd211d1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## UtilityTools\n",
    "Tools to handle large amount of data from other tools. Oftentimes, directly querying an API can return an excessive amount of data, some of which may be irrelevant, overflow the context window of the LLM, or unnecessarily increase the number of tokens that you are using. The main two utility tools are:\n",
    "- `OnDemandToolLoader`: turns an existing LLamaIndex data load into a tool that an agent can use\n",
    "- `LoadAndSearchToolSpec`: takes in any existing Tool as input. As a tool spec, it implements to_tool_list, and when that function is called, two tools are returned: a loading tool and then a search tool. The load Tool execution would call the underlying Tool, and the index the output (by default with a vector index). The search Tool execution would take in a query string as input and call the underlying index."
   ],
   "id": "66c1476e9ff3a78c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
