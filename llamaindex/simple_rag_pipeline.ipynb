{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Simple RAG pipeline\n",
    "Let's first load the dataset finepersonas-v0.1-tiny and save it into the data folder."
   ],
   "id": "7205a9a520eecafe"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "for i, persona in enumerate(dataset):\n",
    "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "        f.write(persona[\"persona\"])"
   ],
   "id": "1c01a60897b506ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now load the persona descriptions from the data directory with a `SimpleDirectoryReader`.",
   "id": "81f15b6909b19f60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "len(documents)"
   ],
   "id": "ed67de78f6754718",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now create a `IngestionPipeline` to create nodes from the documents and divide them into smaller chunks ready for `QueryEngine`. We'll use `SentenceSplitter` to split documents into natural sentences and then `OllamaEmbedding` to create embeddings.",
   "id": "42d4fea0a93e74b4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        OllamaEmbedding(model_name=\"nomic-embed-text\"),\n",
    "        # HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"), # for HuggingFace\n",
    "    ]\n",
    ")\n",
    "\n",
    "# let's now run the pipeline sync or async\n",
    "nodes = await pipeline.arun(documents=documents[:10])\n",
    "nodes"
   ],
   "id": "fcfe07ee7ae426dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll now attach a vector store to the pipeline to populate it. We'll use `Chroma` for this task. Let's run the pipeline again with the vector store attached.",
   "id": "bdbbf1731a0752d9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import chromadb\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "db = chromadb.PersistentClient(\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(name=\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(),\n",
    "        OllamaEmbedding(model_name=\"nomic-embed-text\"),\n",
    "        # HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\"), # for HuggingFace\n",
    "    ],\n",
    "    vector_store=vector_store\n",
    ")\n",
    "\n",
    "nodes = await pipeline.arun(documents=documents[:10])\n",
    "len(nodes)"
   ],
   "id": "eb623412b8d117aa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now create a `VectorStoreIndex` and use it to query the documents by passing the vector store and using the `from_vector_store()` method.",
   "id": "151bced521187d8d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "# for HuggingFace\n",
    "# from llama_index.embeddings.huggingface_api import HuggingFaceInferenceAPIEmbedding\n",
    "# embed_model = HuggingFaceInferenceAPIEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
    "\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    embed_model=embed_model\n",
    ")"
   ],
   "id": "7c7a0ed2adfc9bf7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We'll now query the index by creating a `QueryEngine` from the index.",
   "id": "3dff74667a5af630"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "import nest_asyncio\n",
    "\n",
    "# for HuggingFace\n",
    "# from llama_index.llms.huggingface_api import HuggingFaceInference\n",
    "# llm = HuggingFaceInferenceAPI(model_name=\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n",
    "\n",
    "nest_asyncio.apply()\n",
    "llm = Ollama(model=\"qwen2.5-coder\")\n",
    "query_engine = index.as_query_engine(\n",
    "    llm=llm,\n",
    "    response_mode=\"tree_summarize\"\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"Respond using a persona that describes author and travel experiences?\")\n",
    "response"
   ],
   "id": "b4ce61d02a01eff2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Evaluation and observability\n",
    "Let's now use LlamaIndex built-in evaluation tools to assess the quality of the response. These evaluators leverage LLMs to analyze responses across different dimensions. We can now check if the query is faithful to the original persona."
   ],
   "id": "61dbd8f39479e1a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from llama_index.core.evaluation import FaithfulnessEvaluator\n",
    "\n",
    "evaluator = FaithfulnessEvaluator(llm=llm)\n",
    "eval_result = evaluator.evaluate_response(response=response)\n",
    "eval_result.passing"
   ],
   "id": "565cc5a20053eb3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can also check the response using the Arize Phoenix tool and LlamaTrace.",
   "id": "de7b336713f01afd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import llama_index\n",
    "import os\n",
    "\n",
    "PHOENIX_API_KEY = \"<PHOENIX_API_KEY>\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={PHOENIX_API_KEY}\"\n",
    "llama_index.core.set_global_handler(\n",
    "    \"arize_phoenix\",\n",
    "    endpoint=\"https://llamatrace.com/v1/traces\"\n",
    ")\n",
    "\n",
    "response = query_engine.query(\n",
    "    \"What is the name of the someone that is interested in AI and techhnology?\"\n",
    ")\n",
    "response"
   ],
   "id": "a39d078f1030db89",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
